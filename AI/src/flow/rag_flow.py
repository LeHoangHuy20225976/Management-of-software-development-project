"""
Prefect Flow Ä‘á»ƒ xá»­ lÃ½ RAG: Upload, Chunk, vÃ  Index PDF files
"""
from prefect import flow, task
from pathlib import Path
import tempfile
import shutil
from typing import List
import os


@task(name="save_uploaded_file", retries=2)
def save_uploaded_file(file_content: bytes, filename: str, upload_dir: str) -> str:
    """
    LÆ°u file upload vÃ o thÆ° má»¥c táº¡m
    
    Args:
        file_content: Ná»™i dung file (bytes)
        filename: TÃªn file
        upload_dir: ThÆ° má»¥c lÆ°u file
        
    Returns:
        str: ÄÆ°á»ng dáº«n file Ä‘Ã£ lÆ°u
    """
    # Táº¡o thÆ° má»¥c náº¿u chÆ°a cÃ³
    Path(upload_dir).mkdir(parents=True, exist_ok=True)
    
    # LÆ°u file
    file_path = Path(upload_dir) / filename
    with open(file_path, 'wb') as f:
        f.write(file_content)
    
    print(f"âœ… Saved file: {file_path} ({len(file_content)} bytes)")
    return str(file_path)


@task(name="load_pdf_document", retries=2)
def load_pdf_document(file_path: str):
    """
    Load PDF document báº±ng LlamaIndex
    
    Args:
        file_path: ÄÆ°á»ng dáº«n Ä‘áº¿n PDF file
        
    Returns:
        Document: LlamaIndex document object
    """
    from llama_index.core import SimpleDirectoryReader
    
    print(f"ðŸ“„ Loading PDF: {file_path}")
    
    # Load single file
    reader = SimpleDirectoryReader(
        input_files=[file_path],
        required_exts=[".pdf"]
    )
    documents = reader.load_data()
    
    print(f"âœ… Loaded {len(documents)} document(s)")
    return documents


@task(name="chunk_documents", retries=2)
def chunk_documents(documents, chunk_size: int = 1024, chunk_overlap: int = 200):
    """
    Chia documents thÃ nh chunks nhá»
    
    Args:
        documents: List of LlamaIndex documents
        chunk_size: KÃ­ch thÆ°á»›c má»—i chunk (tokens)
        chunk_overlap: Overlap giá»¯a cÃ¡c chunks
        
    Returns:
        List: List of nodes (chunks)
    """
    from llama_index.core.node_parser import SentenceSplitter
    
    print(f"âœ‚ï¸  Chunking documents (size={chunk_size}, overlap={chunk_overlap})...")
    
    splitter = SentenceSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap
    )
    nodes = splitter.get_nodes_from_documents(documents)
    
    print(f"âœ… Created {len(nodes)} chunks")
    return nodes


@task(name="create_embeddings_and_index", retries=2)
def create_embeddings_and_index(nodes, connection_string: str, table_name: str = "rag_embeddings"):
    """
    Táº¡o embeddings vÃ  index vÃ o PGVector
    
    Args:
        nodes: List of nodes (chunks)
        connection_string: PostgreSQL connection string
        table_name: TÃªn table trong PGVector
        
    Returns:
        dict: Káº¿t quáº£ indexing
    """
    from src.application.services.llm.rag import RAGIndexer
    
    print(f"ðŸ”¢ Creating embeddings and indexing into PGVector...")
    print(f"   Table: {table_name}")
    print(f"   Total chunks: {len(nodes)}")
    
    # Initialize indexer
    indexer = RAGIndexer(
        connection_string=connection_string,
        table_name=table_name,
        embed_dim=384  # all-MiniLM-L6-v2 dimension
    )
    
    # Create index (tá»± Ä‘á»™ng táº¡o embeddings vÃ  insert vÃ o PGVector)
    indexer.create_index(nodes)
    
    print(f"âœ… Indexed {len(nodes)} chunks into PGVector")
    
    return {
        "status": "success",
        "total_chunks": len(nodes),
        "table_name": table_name
    }


@task(name="cleanup_temp_file")
def cleanup_temp_file(file_path: str):
    """XÃ³a file táº¡m sau khi xá»­ lÃ½ xong"""
    try:
        if os.path.exists(file_path):
            os.remove(file_path)
            print(f"ðŸ—‘ï¸  Cleaned up temp file: {file_path}")
    except Exception as e:
        print(f"âš ï¸  Could not delete temp file: {e}")


@flow(name="rag_upload_and_index_flow", log_prints=True)
def rag_upload_and_index_flow(
    file_content: bytes,
    filename: str,
    connection_string: str,
    upload_dir: str = "/tmp/rag_uploads",
    table_name: str = "rag_embeddings",
    chunk_size: int = 1024,
    chunk_overlap: int = 200,
    cleanup: bool = True
):
    """
    Main flow: Upload PDF â†’ Load â†’ Chunk â†’ Create Embeddings â†’ Index to PGVector
    
    Args:
        file_content: Ná»™i dung file PDF (bytes)
        filename: TÃªn file
        connection_string: PostgreSQL connection string
        upload_dir: ThÆ° má»¥c lÆ°u file táº¡m
        table_name: TÃªn table trong PGVector
        chunk_size: KÃ­ch thÆ°á»›c chunk
        chunk_overlap: Overlap giá»¯a chunks
        cleanup: XÃ³a file táº¡m sau khi xong
        
    Returns:
        dict: Káº¿t quáº£ processing
    """
    print("=" * 60)
    print("ðŸš€ RAG UPLOAD AND INDEX FLOW")
    print("=" * 60)
    print(f"File: {filename}")
    print(f"Size: {len(file_content)} bytes")
    print(f"Table: {table_name}")
    print()
    
    # Step 1: Save file
    file_path = save_uploaded_file(file_content, filename, upload_dir)
    
    # Step 2: Load PDF
    documents = load_pdf_document(file_path)
    
    # Step 3: Chunk documents
    nodes = chunk_documents(documents, chunk_size, chunk_overlap)
    
    # Step 4: Create embeddings and index
    result = create_embeddings_and_index(nodes, connection_string, table_name)
    
    # Step 5: Cleanup
    if cleanup:
        cleanup_temp_file(file_path)
    
    print()
    print("=" * 60)
    print("âœ… FLOW COMPLETED SUCCESSFULLY")
    print("=" * 60)
    print(f"File processed: {filename}")
    print(f"Total chunks indexed: {result['total_chunks']}")
    print(f"PGVector table: {result['table_name']}")
    
    return {
        **result,
        "filename": filename,
        "file_size": len(file_content)
    }


# Convenience function Ä‘á»ƒ call tá»« API
def run_rag_indexing(file_content: bytes, filename: str, connection_string: str) -> dict:
    """
    Wrapper function Ä‘á»ƒ cháº¡y flow tá»« API endpoint
    
    Args:
        file_content: Ná»™i dung file PDF
        filename: TÃªn file
        connection_string: PostgreSQL connection string
        
    Returns:
        dict: Káº¿t quáº£ processing
    """
    return rag_upload_and_index_flow(
        file_content=file_content,
        filename=filename,
        connection_string=connection_string
    )
