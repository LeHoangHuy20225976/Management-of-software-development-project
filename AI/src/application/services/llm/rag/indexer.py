from llama_index.core import VectorStoreIndex, StorageContext
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.core.settings import Settings
import chromadb
from pathlib import Path

class RAGIndexer:
    def __init__(self, 
                persist_dir: str = "./chroma_db",
                collection_name: str = "pdf_documents"):
        """
        Initialize RAG Indexer v·ªõi ChromaDB
        
        Args:
            persist_dir: Th∆∞ m·ª•c l∆∞u ChromaDB (persistent storage)
            collection_name: T√™n collection trong ChromaDB
        """
        # T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a c√≥
        self.persist_dir = Path(persist_dir)
        self.persist_dir.mkdir(parents=True, exist_ok=True)
        
        # Kh·ªüi t·∫°o ChromaDB client (persistent)
        self.chroma_client = chromadb.PersistentClient(
            path=str(self.persist_dir)
        )
        
        # T·∫°o/get collection
        self.collection = self.chroma_client.get_or_create_collection(
            name=collection_name
        )
        
        # Setup embedding model - Local HuggingFace (nh·∫π, mi·ªÖn ph√≠)
        self.embed_model = HuggingFaceEmbedding(
            model_name="sentence-transformers/all-MiniLM-L6-v2"  # Model nh·∫π nh·∫•t (~90MB)
        )
        
        # Set global settings (optional)
        Settings.embed_model = self.embed_model
        Settings.chunk_size = 1024
        Settings.chunk_overlap = 200
        
        self.index = None
        
    def create_index(self, nodes):
        """T·∫°o vector index t·ª´ nodes"""
        vector_store = ChromaVectorStore(chroma_collection=self.collection)
    
        # T·∫°o storage context
        storage_context = StorageContext.from_defaults(
            vector_store=vector_store
        )
        
        # T·∫°o index t·ª´ nodes
        self.index = VectorStoreIndex(
            nodes=nodes,
            storage_context=storage_context,
            embed_model=self.embed_model,
            show_progress=True  # Hi·ªÉn th·ªã progress bar
        )
        
        print(f"‚úÖ Created index with {len(nodes)} nodes")
        return self.index
    
    def load_index(self):
        """
        Load index t·ª´ ChromaDB ƒë√£ persist
        
        Returns:
            VectorStoreIndex: Index ƒë√£ load
        """
        # T·∫°o ChromaVectorStore t·ª´ collection ƒë√£ c√≥
        vector_store = ChromaVectorStore(chroma_collection=self.collection)
        
        # Load index t·ª´ vector store
        self.index = VectorStoreIndex.from_vector_store(
            vector_store=vector_store,
            embed_model=self.embed_model
        )
        
        print(f"‚úÖ Loaded index from ChromaDB")
        return self.index
    
    def get_query_engine(self, similarity_top_k: int = 5, response_mode: str = "compact"):
        """
        T·∫°o query engine ƒë·ªÉ search documents
        
        Args:
            similarity_top_k: S·ªë l∆∞·ª£ng chunks relevant tr·∫£ v·ªÅ
            response_mode: compact, tree_summarize, simple_summarize
            
        Returns:
            QueryEngine: Engine ƒë·ªÉ query
        """
        if self.index is None:
            raise ValueError("Index ch∆∞a ƒë∆∞·ª£c t·∫°o. G·ªçi create_index() ho·∫∑c load_index() tr∆∞·ªõc")
        
        query_engine = self.index.as_query_engine(
            similarity_top_k=similarity_top_k,
            response_mode=response_mode
        )
        
        return query_engine
    
    def delete_collection(self):
        """X√≥a collection (ƒë·ªÉ re-index t·ª´ ƒë·∫ßu)"""
        collection_name = self.collection.name
        self.chroma_client.delete_collection(name=collection_name)
        print(f"üóëÔ∏è Deleted collection: {collection_name}")
    
    def get_collection_count(self):
        """ƒê·∫øm s·ªë documents trong collection"""
        count = self.collection.count()
        print(f"üìä Collection has {count} documents")
        return count