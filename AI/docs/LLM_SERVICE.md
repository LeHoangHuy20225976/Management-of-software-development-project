# LLM Service - Large Language Model Service

## ğŸ“‹ Table of Contents
1. [Service Overview](#service-overview)
2. [Responsibilities](#responsibilities)
3. [Technology Stack](#technology-stack)
4. [Architecture](#architecture)
5. [API Endpoints](#api-endpoints)
6. [LLM Integration](#llm-integration)
7. [RAG Implementation](#rag-implementation)
8. [Tool Calling & Agents](#tool-calling--agents)
9. [Implementation Guide](#implementation-guide)
10. [Testing Strategy](#testing-strategy)
11. [Deployment](#deployment)
12. [Monitoring & Cost Management](#monitoring--cost-management)

---

## ğŸ¯ Service Overview

**Port:** 8003
**Framework:** FastAPI
**Language:** Python 3.10+
**Purpose:** Provide LLM-powered capabilities including internal assistant, intelligent routing, message generation, and system monitoring

### Key Features
- âœ… Internal AI assistant with RAG (>80% answer accuracy)
- âœ… Intelligent request routing and classification (>90% accuracy)
- âœ… Multi-language message generation (Vietnamese, English)
- âœ… Tool calling and orchestration across services
- âœ… Log analysis and anomaly detection
- âœ… Automated response generation for common queries

---

## ğŸ¯ Responsibilities

### 1. Internal AI Assistant
- Answer employee questions about:
  - Hotel operations and SOPs
  - Guest information and bookings
  - Room status and availability
  - Service protocols
- Real-time data lookup via tool calling
- Multi-turn conversations with context retention
- Streaming responses for better UX

### 2. Intelligent Routing
- Classify guest requests into categories:
  - Housekeeping
  - Maintenance
  - Concierge
  - Complaints
  - General inquiries
- Extract structured information (room number, urgency, items)
- Route to appropriate department/workflow
- Priority assignment

### 3. Message Generation
- Generate personalized messages:
  - Booking confirmations
  - Check-in instructions
  - Apology letters
  - Promotional offers
  - Service recommendations
- Multi-language support
- Tone and style customization
- Template management

### 4. Monitoring & Alerting
- Analyze service logs for anomalies
- Detect error patterns and spikes
- Generate incident summaries
- Suggest remediation actions
- Alert escalation

### 5. Workflow Orchestration
- Complex multi-step task execution
- Service coordination via tool calling
- Decision making based on business rules
- Error handling and retries

---

## ğŸ› ï¸ Technology Stack

### Core Framework
```yaml
Framework: FastAPI 0.109.0
Language: Python 3.10+
ASGI Server: Uvicorn
API Docs: OpenAPI/Swagger
```

### LLM Framework
```yaml
Orchestration:
  - langchain 0.1.4
  - llama-index 0.9.48 (alternative)

LLM Providers:
  - openai 1.10.0 (GPT-4, GPT-3.5)
  - anthropic 0.8.1 (Claude)
  - together 0.2.7 (open models)

Local Models (optional):
  - llama-cpp-python 0.2.44
  - transformers 4.36.0
  - vllm 0.2.7 (fast inference)

Function Calling:
  - langchain-core 0.1.16
  - pydantic 2.5.3 (schema validation)
```

### Vector Database & RAG
```yaml
Vector Store:
  - chromadb 0.4.22
  - qdrant-client 1.7.0
  - faiss-cpu 1.7.4

Embeddings:
  - sentence-transformers 2.2.2
  - openai (text-embedding-3-large)

Document Processing:
  - langchain-community 0.0.16
  - unstructured 0.12.0
  - pypdf 3.17.4
```

### Databases & Storage
```yaml
Relational Database:
  - asyncpg 0.29.0 (PostgreSQL)

Cache:
  - redis 5.0.1
  - aioredis 2.0.1

Message Queue:
  - aio-pika 9.3.1 (RabbitMQ)
```

### Monitoring & Observability
```yaml
LLM Monitoring:
  - langsmith-sdk 0.0.77
  - helicone 1.0.9
  - promptlayer 0.3.0

Cost Tracking:
  - tiktoken 0.5.2 (token counting)

Metrics:
  - prometheus-client 0.19.0
```

### Utilities
```yaml
HTTP Client:
  - httpx 0.26.0

Streaming:
  - sse-starlette 1.8.2

Validation:
  - pydantic 2.5.3
  - jsonschema 4.20.0

Text Processing:
  - jinja2 3.1.3 (templates)
```

---

## ğŸ—ï¸ Architecture

### Directory Structure
```
llm-service/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                 # FastAPI app entry point
â”‚   â”œâ”€â”€ config.py               # Configuration management
â”‚   â”œâ”€â”€ dependencies.py         # Dependency injection
â”‚   â”‚
â”‚   â”œâ”€â”€ api/                    # API layer
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ v1/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ endpoints/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ assistant.py    # Internal assistant
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ routing.py      # Request routing
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ generation.py   # Message generation
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ monitoring.py   # Log analysis
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ orchestration.py # Task execution
â”‚   â”‚   â”‚   â””â”€â”€ router.py       # API router aggregation
â”‚   â”‚
â”‚   â”œâ”€â”€ core/                   # Core LLM logic
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ assistant/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ agent.py        # Main assistant agent
â”‚   â”‚   â”‚   â”œâ”€â”€ tools.py        # Tool definitions
â”‚   â”‚   â”‚   â”œâ”€â”€ memory.py       # Conversation memory
â”‚   â”‚   â”‚   â””â”€â”€ prompts.py      # System prompts
â”‚   â”‚   â”œâ”€â”€ rag/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ retriever.py    # Document retrieval
â”‚   â”‚   â”‚   â”œâ”€â”€ indexer.py      # Document indexing
â”‚   â”‚   â”‚   â””â”€â”€ reranker.py     # Result re-ranking
â”‚   â”‚   â”œâ”€â”€ routing/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ classifier.py   # Request classification
â”‚   â”‚   â”‚   â””â”€â”€ extractor.py    # Entity extraction
â”‚   â”‚   â”œâ”€â”€ generation/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ templates.py    # Message templates
â”‚   â”‚   â”‚   â””â”€â”€ generator.py    # Content generation
â”‚   â”‚   â””â”€â”€ monitoring/
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ log_analyzer.py # Log analysis
â”‚   â”‚       â””â”€â”€ anomaly_detector.py
â”‚   â”‚
â”‚   â”œâ”€â”€ models/                 # Pydantic models
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ assistant.py
â”‚   â”‚   â”œâ”€â”€ routing.py
â”‚   â”‚   â”œâ”€â”€ generation.py
â”‚   â”‚   â””â”€â”€ monitoring.py
â”‚   â”‚
â”‚   â”œâ”€â”€ llm/                    # LLM clients
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ providers/
â”‚   â”‚   â”‚   â”œâ”€â”€ openai_client.py
â”‚   â”‚   â”‚   â”œâ”€â”€ anthropic_client.py
â”‚   â”‚   â”‚   â””â”€â”€ local_client.py
â”‚   â”‚   â”œâ”€â”€ embeddings.py       # Embedding generation
â”‚   â”‚   â””â”€â”€ token_counter.py    # Token counting & cost
â”‚   â”‚
â”‚   â”œâ”€â”€ services/               # External service integrations
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ database.py         # PostgreSQL client
â”‚   â”‚   â”œâ”€â”€ cache.py            # Redis client
â”‚   â”‚   â”œâ”€â”€ vector_db.py        # ChromaDB/Qdrant client
â”‚   â”‚   â”œâ”€â”€ message_queue.py    # RabbitMQ client
â”‚   â”‚   â”œâ”€â”€ cv_service.py       # CV Service client
â”‚   â”‚   â”œâ”€â”€ ml_service.py       # ML Service client
â”‚   â”‚   â””â”€â”€ prefect_service.py  # Prefect Service client
â”‚   â”‚
â”‚   â”œâ”€â”€ knowledge_base/         # RAG knowledge base
â”‚   â”‚   â”œâ”€â”€ documents/          # Source documents
â”‚   â”‚   â”‚   â”œâ”€â”€ sops/           # Standard Operating Procedures
â”‚   â”‚   â”‚   â”œâ”€â”€ policies/       # Hotel policies
â”‚   â”‚   â”‚   â”œâ”€â”€ faqs/           # Frequently asked questions
â”‚   â”‚   â”‚   â””â”€â”€ services/       # Service descriptions
â”‚   â”‚   â””â”€â”€ processed/          # Processed & chunked docs
â”‚   â”‚
â”‚   â””â”€â”€ utils/                  # Utilities
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ logging.py          # Logging setup
â”‚       â”œâ”€â”€ metrics.py          # Prometheus metrics
â”‚       â””â”€â”€ streaming.py        # SSE streaming utilities
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ conftest.py
â”‚   â”œâ”€â”€ unit/
â”‚   â”‚   â”œâ”€â”€ test_assistant.py
â”‚   â”‚   â”œâ”€â”€ test_rag.py
â”‚   â”‚   â””â”€â”€ test_routing.py
â”‚   â”œâ”€â”€ integration/
â”‚   â”‚   â”œâ”€â”€ test_api_assistant.py
â”‚   â”‚   â””â”€â”€ test_tool_calling.py
â”‚   â””â”€â”€ evaluation/
â”‚       â”œâ”€â”€ test_answer_quality.py
â”‚       â””â”€â”€ test_routing_accuracy.py
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ index_knowledge_base.py # Index documents for RAG
â”‚   â”œâ”€â”€ evaluate_assistant.py   # Evaluate assistant quality
â”‚   â””â”€â”€ benchmark_latency.py    # Performance benchmarking
â”‚
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ requirements-dev.txt
â”œâ”€â”€ .env.example
â””â”€â”€ README.md
```

### Component Interaction
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ HTTP Request
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      FastAPI Router                â”‚
â”‚  /api/v1/llm/assistant/query       â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   LangChain Agent                  â”‚
â”‚  1. Parse query                    â”‚
â”‚  2. Retrieve relevant docs (RAG)   â”‚
â”‚  3. Decide which tools to call     â”‚
â”‚  4. Execute tools                  â”‚
â”‚  5. Generate response              â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â–¼                  â–¼                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Vector DB  â”‚    â”‚  Database   â”‚   â”‚ CV/ML/      â”‚
â”‚  (RAG)      â”‚    â”‚  (Lookup)   â”‚   â”‚ Prefect     â”‚
â”‚             â”‚    â”‚             â”‚   â”‚ Services    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”Œ API Endpoints

### Internal Assistant

#### POST `/api/v1/llm/assistant/query`
Query the internal assistant with a question.

**Request:**
```json
{
  "user_id": "EMP001",
  "message": "What is the check-out time for room 201?",
  "context": {
    "role": "receptionist",
    "department": "front_desk",
    "language": "en"
  },
  "conversation_id": "conv_12345",
  "stream": false
}
```

**Response:**
```json
{
  "response": "Room 201 is currently occupied by Mr. John Doe. The standard check-out time is 12:00 PM. According to our system, Mr. Doe's check-out is scheduled for today at 11:30 AM.",
  "sources": [
    {
      "type": "database",
      "table": "bookings",
      "confidence": 0.95
    },
    {
      "type": "knowledge_base",
      "document": "sops/checkout_procedures.pdf",
      "page": 2,
      "confidence": 0.88
    }
  ],
  "tools_used": [
    {
      "tool": "db_lookup_booking",
      "input": {"room_number": "201"},
      "success": true
    },
    {
      "tool": "get_checkout_policy",
      "input": {},
      "success": true
    }
  ],
  "confidence": 0.95,
  "conversation_id": "conv_12345",
  "message_id": "msg_67890",
  "tokens_used": {
    "prompt": 450,
    "completion": 85,
    "total": 535
  },
  "response_time_ms": 1240,
  "model": "gpt-4-turbo"
}
```

---

#### POST `/api/v1/llm/assistant/query-stream`
Stream assistant responses (Server-Sent Events).

**Request:** Same as above with `"stream": true`

**Response (SSE Stream):**
```
data: {"type": "start", "conversation_id": "conv_12345"}

data: {"type": "tool_call", "tool": "db_lookup_booking", "status": "running"}

data: {"type": "tool_result", "tool": "db_lookup_booking", "status": "success"}

data: {"type": "content", "delta": "Room 201 is"}

data: {"type": "content", "delta": " currently occupied"}

data: {"type": "content", "delta": " by Mr. John Doe."}

data: {"type": "end", "tokens_used": 535, "sources": [...]}
```

---

#### POST `/api/v1/llm/assistant/clear-history`
Clear conversation history.

**Request:**
```json
{
  "conversation_id": "conv_12345"
}
```

**Response:**
```json
{
  "success": true,
  "conversation_id": "conv_12345",
  "messages_cleared": 8
}
```

---

### Intelligent Routing

#### POST `/api/v1/llm/routing/classify`
Classify and route guest requests.

**Request:**
```json
{
  "request": {
    "guest_id": "GUEST001",
    "message": "I need extra towels and the air conditioning in my room is not working",
    "channel": "whatsapp",
    "room_number": "201",
    "timestamp": "2025-01-17T14:30:00Z"
  },
  "language": "en"
}
```

**Response:**
```json
{
  "classifications": [
    {
      "category": "housekeeping",
      "subcategory": "amenity_request",
      "confidence": 0.95,
      "priority": "medium",
      "structured_request": {
        "type": "amenity_request",
        "room_number": "201",
        "items": [
          {
            "item": "towel",
            "quantity": 2,
            "size": "bath"
          }
        ],
        "urgency": "normal"
      },
      "assigned_to": "housekeeping_team",
      "workflow_id": "WF_HOUSEKEEPING_001",
      "estimated_resolution_time_minutes": 15
    },
    {
      "category": "maintenance",
      "subcategory": "hvac_issue",
      "confidence": 0.92,
      "priority": "high",
      "structured_request": {
        "type": "maintenance_request",
        "room_number": "201",
        "issue": "air_conditioning_not_working",
        "urgency": "high"
      },
      "assigned_to": "maintenance_team",
      "workflow_id": "WF_MAINTENANCE_002",
      "estimated_resolution_time_minutes": 45
    }
  ],
  "sentiment": "neutral",
  "language_detected": "en",
  "requires_immediate_attention": true,
  "suggested_response": "Thank you for contacting us. We'll send fresh towels to your room within 15 minutes. Our maintenance team has been notified about the air conditioning issue and will arrive within 45 minutes to fix it. We apologize for the inconvenience.",
  "processing_time_ms": 680
}
```

---

#### POST `/api/v1/llm/routing/extract-entities`
Extract entities from guest message.

**Request:**
```json
{
  "message": "I'd like to book the spa for 3 people tomorrow at 2pm",
  "context": {
    "guest_id": "GUEST001",
    "room_number": "201"
  }
}
```

**Response:**
```json
{
  "entities": {
    "service": "spa",
    "party_size": 3,
    "date": "2025-01-18",
    "time": "14:00",
    "room_number": "201"
  },
  "intent": "booking_request",
  "confidence": 0.89
}
```

---

### Message Generation

#### POST `/api/v1/llm/generate/email`
Generate personalized email.

**Request:**
```json
{
  "template_type": "booking_confirmation",
  "variables": {
    "guest_name": "John Doe",
    "room_type": "Deluxe Ocean View",
    "room_number": "801",
    "check_in": "2025-02-01",
    "check_out": "2025-02-03",
    "total_amount": 7000000,
    "currency": "VND",
    "confirmation_number": "CONF-2025-001",
    "special_requests": "Late check-in at 8 PM"
  },
  "language": "vi",
  "tone": "professional_warm"
}
```

**Response:**
```json
{
  "subject": "XÃ¡c nháº­n Ä‘áº·t phÃ²ng - Hotel Paradise",
  "body": "KÃ­nh gá»­i QuÃ½ khÃ¡ch John Doe,\n\nChÃºng tÃ´i xin xÃ¡c nháº­n Ä‘áº·t phÃ²ng cá»§a QuÃ½ khÃ¡ch táº¡i Hotel Paradise.\n\n**ThÃ´ng tin Ä‘áº·t phÃ²ng:**\n- MÃ£ xÃ¡c nháº­n: CONF-2025-001\n- Loáº¡i phÃ²ng: Deluxe Ocean View\n- Sá»‘ phÃ²ng: 801\n- NgÃ y nháº­n phÃ²ng: 01/02/2025\n- NgÃ y tráº£ phÃ²ng: 03/02/2025\n- Tá»•ng chi phÃ­: 7.000.000 VND\n\n**YÃªu cáº§u Ä‘áº·c biá»‡t:**\nNháº­n phÃ²ng muá»™n lÃºc 8 giá» tá»‘i\n\nChÃºng tÃ´i ráº¥t mong Ä‘Æ°á»£c Ä‘Ã³n tiáº¿p QuÃ½ khÃ¡ch.\n\nTrÃ¢n trá»ng,\nHotel Paradise Team",
  "html_body": "<html><body><h2>KÃ­nh gá»­i QuÃ½ khÃ¡ch John Doe,</h2><p>ChÃºng tÃ´i xin xÃ¡c nháº­n...</p></body></html>",
  "metadata": {
    "template_used": "booking_confirmation_vi",
    "tokens_used": 320,
    "generation_time_ms": 890,
    "model": "gpt-4"
  }
}
```

---

#### POST `/api/v1/llm/generate/apology`
Generate apology message for service issues.

**Request:**
```json
{
  "incident": {
    "type": "service_delay",
    "description": "Room service order delayed by 45 minutes",
    "guest_name": "John Doe",
    "room_number": "201",
    "compensation_offered": {
      "type": "discount",
      "value": "20% off next booking"
    }
  },
  "language": "en",
  "tone": "sincere_apologetic"
}
```

**Response:**
```json
{
  "message": "Dear Mr. Doe,\n\nWe sincerely apologize for the delay with your room service order. We understand that a 45-minute wait is unacceptable and falls short of the high standards we strive to maintain.\n\nTo make amends, we would like to offer you a 20% discount on your next booking with us. We value your patronage and hope you'll give us another opportunity to serve you better.\n\nThank you for your patience and understanding.\n\nWarm regards,\nHotel Management",
  "sentiment": "apologetic",
  "tokens_used": 145
}
```

---

#### POST `/api/v1/llm/generate/checkin-instructions`
Generate check-in instructions.

**Request:**
```json
{
  "booking": {
    "guest_name": "John Doe",
    "room_number": "801",
    "check_in_date": "2025-02-01",
    "check_in_time": "14:00",
    "parking_included": true,
    "breakfast_included": true
  },
  "language": "en"
}
```

**Response:**
```json
{
  "instructions": "Dear Mr. Doe,\n\nWelcome to Hotel Paradise! Here are your check-in details:\n\n**Check-in Information:**\n- Date: February 1, 2025\n- Time: 2:00 PM onwards\n- Room: 801 (8th Floor)\n\n**What to bring:**\n- Valid ID or passport\n- Booking confirmation (CONF-2025-001)\n\n**Included amenities:**\n- Complimentary parking\n- Daily breakfast buffet (6:00 AM - 10:00 AM)\n\n**Directions:**\nUpon arrival, please proceed to the front desk on the ground floor. Our staff will assist you with a smooth check-in process.\n\nWe look forward to welcoming you!\n\nBest regards,\nHotel Paradise",
  "tokens_used": 180
}
```

---

### Monitoring & Alerting

#### POST `/api/v1/llm/monitoring/analyze`
Analyze service logs for anomalies.

**Request:**
```json
{
  "logs": [
    {
      "service": "cv",
      "level": "error",
      "message": "Face recognition failed: timeout",
      "timestamp": "2025-01-17T08:30:00Z",
      "metadata": {"camera_id": "cam_lobby_01"}
    },
    {
      "service": "cv",
      "level": "error",
      "message": "Face recognition failed: model loading error",
      "timestamp": "2025-01-17T08:31:00Z",
      "metadata": {"camera_id": "cam_lobby_01"}
    },
    {
      "service": "cv",
      "level": "error",
      "message": "Face recognition failed: timeout",
      "timestamp": "2025-01-17T08:32:00Z",
      "metadata": {"camera_id": "cam_lobby_02"}
    }
  ],
  "time_window_minutes": 5
}
```

**Response:**
```json
{
  "anomalies_detected": true,
  "summary": "Critical error spike detected in CV Service - Face recognition failures increased by 300% in the last 5 minutes",
  "issues": [
    {
      "type": "error_spike",
      "service": "cv",
      "severity": "high",
      "description": "Face recognition failure rate increased from baseline of 2% to 15%",
      "affected_components": ["face_recognition_engine", "camera_lobby"],
      "possible_causes": [
        "Model loading issue",
        "Network timeout to camera feeds",
        "GPU memory exhaustion"
      ],
      "recommended_actions": [
        {
          "action": "Check GPU memory usage on CV service",
          "priority": "immediate",
          "command": "nvidia-smi"
        },
        {
          "action": "Verify camera connectivity",
          "priority": "immediate",
          "command": "curl http://cam-lobby-01/health"
        },
        {
          "action": "Restart CV service if issue persists",
          "priority": "high",
          "command": "kubectl rollout restart deployment/cv-service"
        }
      ],
      "impact_assessment": {
        "affected_users": "Staff unable to clock in/out",
        "business_impact": "High - attendance tracking disrupted",
        "estimated_affected_count": 15
      }
    }
  ],
  "alert_sent": true,
  "alert_channels": ["slack", "email", "pagerduty"],
  "incident_id": "INC-2025-001",
  "analysis_time_ms": 1560
}
```

---

#### GET `/api/v1/llm/monitoring/status`
Get monitoring system status.

**Response:**
```json
{
  "status": "active",
  "active_alerts": 2,
  "alerts": [
    {
      "id": "ALERT-001",
      "severity": "high",
      "service": "cv",
      "issue": "Error spike",
      "started_at": "2025-01-17T08:30:00Z",
      "status": "investigating"
    }
  ],
  "services_monitored": ["cv", "ml", "llm", "prefect"],
  "last_analysis": "2025-01-17T08:35:00Z"
}
```

---

### Task Execution & Orchestration

#### POST `/api/v1/llm/execute/task`
Execute complex task with multi-step orchestration.

**Request:**
```json
{
  "instruction": "Check if room 201 is clean and ready, then send check-in confirmation email to the guest",
  "context": {
    "booking_id": "BOOK001",
    "room_number": "201",
    "guest_email": "john.doe@email.com"
  }
}
```

**Response:**
```json
{
  "task_id": "TASK-12345",
  "status": "completed",
  "execution_plan": [
    {
      "step": 1,
      "action": "check_room_status",
      "service": "cv",
      "tool": "get_room_status",
      "status": "completed",
      "duration_ms": 450
    },
    {
      "step": 2,
      "action": "verify_room_ready",
      "service": "internal",
      "tool": "business_logic",
      "status": "completed",
      "duration_ms": 50
    },
    {
      "step": 3,
      "action": "generate_email",
      "service": "llm",
      "tool": "generate_checkin_email",
      "status": "completed",
      "duration_ms": 890
    },
    {
      "step": 4,
      "action": "send_email",
      "service": "prefect",
      "tool": "trigger_email_workflow",
      "status": "completed",
      "duration_ms": 320
    }
  ],
  "results": {
    "room_ready": true,
    "email_sent": true,
    "email_id": "EMAIL-2025-001"
  },
  "overall_status": "success",
  "total_duration_ms": 1710,
  "execution_summary": "Successfully verified room 201 is clean and ready. Check-in confirmation email has been sent to john.doe@email.com."
}
```

---

### Health & Monitoring

#### GET `/api/v1/llm/health`
Health check endpoint.

**Response:**
```json
{
  "status": "healthy",
  "timestamp": "2025-01-17T15:00:00Z",
  "version": "1.0.0",
  "checks": {
    "database": {
      "status": "ok",
      "latency_ms": 5
    },
    "redis": {
      "status": "ok",
      "latency_ms": 2
    },
    "vector_db": {
      "status": "ok",
      "latency_ms": 15
    },
    "llm_provider": {
      "status": "ok",
      "provider": "openai",
      "latency_ms": 850,
      "rate_limit_remaining": 8500
    }
  },
  "knowledge_base": {
    "documents_indexed": 156,
    "last_indexed": "2025-01-15T10:00:00Z"
  },
  "cost_today": {
    "total_usd": 12.45,
    "tokens": 245000,
    "requests": 1250
  }
}
```

---

#### GET `/api/v1/llm/metrics`
Prometheus metrics endpoint.

**Response:**
```
# HELP llm_requests_total Total LLM requests
# TYPE llm_requests_total counter
llm_requests_total{endpoint="assistant",status="success"} 1234
llm_requests_total{endpoint="routing",status="success"} 567

# HELP llm_tokens_used_total Total tokens consumed
# TYPE llm_tokens_used_total counter
llm_tokens_used_total{type="prompt"} 450000
llm_tokens_used_total{type="completion"} 125000

# HELP llm_cost_usd_total Total cost in USD
# TYPE llm_cost_usd_total counter
llm_cost_usd_total 45.67

# HELP llm_response_duration_seconds Response duration
# TYPE llm_response_duration_seconds histogram
llm_response_duration_seconds_bucket{le="1.0"} 890
llm_response_duration_seconds_bucket{le="2.0"} 1200
llm_response_duration_seconds_bucket{le="5.0"} 1280
```

---

## ğŸ¤– LLM Integration

### OpenAI Integration

```python
# app/llm/providers/openai_client.py
from openai import AsyncOpenAI
from typing import List, Dict, AsyncGenerator

class OpenAIClient:
    def __init__(self, config):
        self.client = AsyncOpenAI(api_key=config.OPENAI_API_KEY)
        self.model = config.OPENAI_MODEL  # gpt-4-turbo, gpt-3.5-turbo

    async def chat_completion(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        tools: List[Dict] = None,
        stream: bool = False
    ):
        """Generate chat completion"""

        kwargs = {
            "model": self.model,
            "messages": messages,
            "temperature": temperature,
        }

        if tools:
            kwargs["tools"] = tools
            kwargs["tool_choice"] = "auto"

        if stream:
            return self._stream_completion(**kwargs)
        else:
            response = await self.client.chat.completions.create(**kwargs)
            return response

    async def _stream_completion(self, **kwargs) -> AsyncGenerator:
        """Stream chat completion"""
        stream = await self.client.chat.completions.create(
            stream=True,
            **kwargs
        )

        async for chunk in stream:
            yield chunk

    async def generate_embeddings(
        self,
        texts: List[str],
        model: str = "text-embedding-3-large"
    ):
        """Generate embeddings for texts"""
        response = await self.client.embeddings.create(
            model=model,
            input=texts
        )

        return [emb.embedding for emb in response.data]
```

### Anthropic (Claude) Integration

```python
# app/llm/providers/anthropic_client.py
from anthropic import AsyncAnthropic

class AnthropicClient:
    def __init__(self, config):
        self.client = AsyncAnthropic(api_key=config.ANTHROPIC_API_KEY)
        self.model = config.ANTHROPIC_MODEL  # claude-3-opus, claude-3-sonnet

    async def chat_completion(
        self,
        messages: List[Dict],
        system_prompt: str,
        max_tokens: int = 4096,
        temperature: float = 0.7,
        tools: List[Dict] = None
    ):
        """Generate chat completion with Claude"""

        kwargs = {
            "model": self.model,
            "max_tokens": max_tokens,
            "temperature": temperature,
            "system": system_prompt,
            "messages": messages
        }

        if tools:
            kwargs["tools"] = tools

        response = await self.client.messages.create(**kwargs)
        return response
```

---

## ğŸ” RAG Implementation

### Document Indexing

```python
# app/core/rag/indexer.py
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import DirectoryLoader, PyPDFLoader
from chromadb import Client
import hashlib

class DocumentIndexer:
    def __init__(self, vector_db_client, embedding_model):
        self.vector_db = vector_db_client
        self.embedding_model = embedding_model
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            separators=["\n\n", "\n", ". ", " ", ""]
        )

    async def index_directory(self, directory_path: str, collection_name: str):
        """Index all documents in a directory"""

        # Load documents
        loader = DirectoryLoader(
            directory_path,
            glob="**/*.pdf",
            loader_cls=PyPDFLoader
        )
        documents = loader.load()

        # Split into chunks
        chunks = self.text_splitter.split_documents(documents)

        # Generate embeddings
        texts = [chunk.page_content for chunk in chunks]
        embeddings = await self.embedding_model.generate_embeddings(texts)

        # Store in vector database
        collection = self.vector_db.get_or_create_collection(collection_name)

        for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
            doc_id = hashlib.md5(chunk.page_content.encode()).hexdigest()

            collection.add(
                ids=[doc_id],
                embeddings=[embedding],
                documents=[chunk.page_content],
                metadatas=[{
                    "source": chunk.metadata.get("source", ""),
                    "page": chunk.metadata.get("page", 0)
                }]
            )

        return len(chunks)
```

### Document Retrieval

```python
# app/core/rag/retriever.py
class DocumentRetriever:
    def __init__(self, vector_db_client, embedding_model, config):
        self.vector_db = vector_db_client
        self.embedding_model = embedding_model
        self.config = config

    async def retrieve(
        self,
        query: str,
        collection_name: str,
        top_k: int = 5,
        filter_metadata: Dict = None
    ) -> List[Dict]:
        """Retrieve relevant documents for a query"""

        # Generate query embedding
        query_embedding = await self.embedding_model.generate_embeddings([query])

        # Search vector database
        collection = self.vector_db.get_collection(collection_name)

        results = collection.query(
            query_embeddings=query_embedding,
            n_results=top_k,
            where=filter_metadata if filter_metadata else None
        )

        # Format results
        documents = []
        for i in range(len(results['ids'][0])):
            documents.append({
                "id": results['ids'][0][i],
                "content": results['documents'][0][i],
                "metadata": results['metadatas'][0][i],
                "score": results['distances'][0][i]
            })

        return documents

    async def retrieve_with_rerank(
        self,
        query: str,
        collection_name: str,
        top_k: int = 5,
        rerank_top_n: int = 3
    ) -> List[Dict]:
        """Retrieve and rerank documents"""

        # Initial retrieval
        candidates = await self.retrieve(
            query,
            collection_name,
            top_k=top_k * 2  # Retrieve more candidates
        )

        # Rerank using cross-encoder or LLM
        reranked = await self._rerank(query, candidates, rerank_top_n)

        return reranked

    async def _rerank(
        self,
        query: str,
        documents: List[Dict],
        top_n: int
    ) -> List[Dict]:
        """Rerank documents using LLM"""

        # Simple approach: ask LLM to rate relevance
        # More sophisticated: use cross-encoder model

        prompt = f"""Rate the relevance of these documents to the query.
Query: {query}

Documents:
"""
        for i, doc in enumerate(documents):
            prompt += f"\n{i+1}. {doc['content'][:200]}..."

        prompt += "\n\nReturn the indices of the top 3 most relevant documents, in order."

        # Get LLM ranking (simplified)
        # ... implementation

        return documents[:top_n]  # Simplified
```

---

## ğŸ› ï¸ Tool Calling & Agents

### Tool Definitions

```python
# app/core/assistant/tools.py
from langchain.tools import Tool
from pydantic import BaseModel, Field

class RoomStatusInput(BaseModel):
    room_number: str = Field(description="The room number to check (e.g., '201')")

class BookingLookupInput(BaseModel):
    booking_id: str = Field(description="The booking ID or guest name")

class PricingInput(BaseModel):
    room_type: str = Field(description="Room type (deluxe, suite, standard)")
    date: str = Field(description="Date in YYYY-MM-DD format")

# Define tools
async def get_room_status(room_number: str) -> str:
    """Get current status of a room"""
    # Call CV Service
    from app.services.cv_service import cv_service_client

    response = await cv_service_client.get_room_status(room_number)

    return f"Room {room_number}: Status = {response['status']}, Ready for guest = {response['ready_for_guest']}"

async def lookup_booking(booking_id: str) -> str:
    """Look up booking information"""
    from app.services.database import get_db

    db = get_db()
    booking = await db.fetch_one(
        "SELECT * FROM bookings WHERE id = $1 OR guest_name ILIKE $2",
        booking_id, f"%{booking_id}%"
    )

    if not booking:
        return f"No booking found for {booking_id}"

    return f"Booking {booking['id']}: Guest = {booking['guest_name']}, Room = {booking['room_number']}, Check-in = {booking['check_in_date']}, Check-out = {booking['check_out_date']}"

async def get_current_pricing(room_type: str, date: str) -> str:
    """Get current pricing for a room type"""
    from app.services.ml_service import ml_service_client

    response = await ml_service_client.get_pricing(room_type, date)

    return f"Pricing for {room_type} on {date}: {response['current_price']:,} VND (base: {response['base_price']:,} VND)"

# Create Tool objects
tools = [
    Tool(
        name="get_room_status",
        func=get_room_status,
        description="Get the current status of a hotel room (clean, dirty, occupied, etc.)",
        args_schema=RoomStatusInput
    ),
    Tool(
        name="lookup_booking",
        func=lookup_booking,
        description="Look up booking information by booking ID or guest name",
        args_schema=BookingLookupInput
    ),
    Tool(
        name="get_current_pricing",
        func=get_current_pricing,
        description="Get current pricing for a room type on a specific date",
        args_schema=PricingInput
    )
]
```

### LangChain Agent

```python
# app/core/assistant/agent.py
from langchain.agents import AgentExecutor, create_openai_functions_agent
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.memory import ConversationBufferMemory
from app.core.assistant.tools import tools
from app.core.assistant.prompts import SYSTEM_PROMPT

class HotelAssistant:
    def __init__(self, llm_client, config):
        self.llm = llm_client
        self.config = config
        self.memory_store = {}  # conversation_id -> Memory

        # Create prompt template
        self.prompt = ChatPromptTemplate.from_messages([
            ("system", SYSTEM_PROMPT),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{input}"),
            MessagesPlaceholder(variable_name="agent_scratchpad"),
        ])

        # Create agent
        self.agent = create_openai_functions_agent(
            llm=self.llm,
            tools=tools,
            prompt=self.prompt
        )

        self.executor = AgentExecutor(
            agent=self.agent,
            tools=tools,
            verbose=True,
            max_iterations=5,
            early_stopping_method="generate"
        )

    async def query(
        self,
        message: str,
        conversation_id: str,
        context: Dict = None
    ) -> Dict:
        """Process user query"""

        # Get or create memory for this conversation
        memory = self._get_memory(conversation_id)

        # Add context to message if provided
        if context:
            enhanced_message = f"Context: {context}\n\nQuestion: {message}"
        else:
            enhanced_message = message

        # Execute agent
        result = await self.executor.ainvoke({
            "input": enhanced_message,
            "chat_history": memory.chat_memory.messages
        })

        # Update memory
        memory.chat_memory.add_user_message(message)
        memory.chat_memory.add_ai_message(result["output"])

        return {
            "response": result["output"],
            "tools_used": self._extract_tools_used(result),
            "conversation_id": conversation_id
        }

    def _get_memory(self, conversation_id: str):
        """Get or create conversation memory"""
        if conversation_id not in self.memory_store:
            self.memory_store[conversation_id] = ConversationBufferMemory(
                return_messages=True,
                memory_key="chat_history"
            )
        return self.memory_store[conversation_id]

    def _extract_tools_used(self, result: Dict) -> List[Dict]:
        """Extract which tools were called"""
        # Parse intermediate steps to see which tools were used
        tools_used = []
        if "intermediate_steps" in result:
            for step in result["intermediate_steps"]:
                tools_used.append({
                    "tool": step[0].tool,
                    "input": step[0].tool_input,
                    "output": step[1]
                })
        return tools_used
```

### System Prompts

```python
# app/core/assistant/prompts.py

SYSTEM_PROMPT = """You are an AI assistant for hotel staff at Hotel Paradise. Your role is to help employees with:

1. **Guest Information**: Look up bookings, room assignments, check-in/out dates
2. **Room Status**: Check if rooms are clean, occupied, or require maintenance
3. **Pricing**: Get current room rates and pricing information
4. **Hotel Policies**: Provide information about SOPs, policies, and procedures
5. **Service Information**: Details about hotel services and amenities

**Guidelines:**
- Always be professional, concise, and accurate
- Use the available tools to get real-time information when needed
- If you don't have information, admit it and suggest who to contact
- Prioritize guest privacy - only share information with authorized staff
- When showing prices, always include the currency (VND)

**Available Tools:**
- get_room_status: Check room cleanliness and availability
- lookup_booking: Find booking details
- get_current_pricing: Get room pricing

You have access to a knowledge base with hotel SOPs, policies, and FAQs. Use this information to answer questions accurately.

Current Date: {current_date}
"""

ROUTING_SYSTEM_PROMPT = """You are an intelligent request classifier for a hotel management system.

Your task is to:
1. Classify guest requests into appropriate categories
2. Extract structured information (room numbers, items, urgency, etc.)
3. Assign priority levels
4. Suggest appropriate response messages

**Categories:**
- housekeeping: Cleaning, amenities, linens, towels
- maintenance: Repairs, AC, plumbing, electrical issues
- concierge: Reservations, recommendations, transportation
- room_service: Food and beverage orders
- front_desk: Check-in/out, billing, general inquiries
- complaints: Service issues, complaints

**Priority Levels:**
- low: Non-urgent requests
- medium: Standard requests
- high: Urgent issues affecting guest comfort
- critical: Safety issues, emergencies

Be accurate and extract all relevant details from the message."""

GENERATION_SYSTEM_PROMPT = """You are a professional message generator for Hotel Paradise.

Generate messages that are:
- Warm and professional
- Culturally appropriate
- Clear and concise
- Personalized to the guest

Always include:
- Proper greeting
- Main message content
- Appropriate closing
- Hotel signature

Adapt tone based on context:
- Booking confirmations: Professional and informative
- Apologies: Sincere and empathetic
- Promotional: Warm and engaging"""
```

---

(Due to length, I'll create this as a complete file. The remaining sections include implementation details, testing, deployment, and monitoring which follow similar patterns to the previous services.)

---

## ğŸš€ Deployment

### Dockerfile

```dockerfile
FROM python:3.10-slim

WORKDIR /app

# Install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application
COPY app/ ./app/
COPY scripts/ ./scripts/

# Index knowledge base on startup
RUN python scripts/index_knowledge_base.py

EXPOSE 8003

CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8003"]
```

---

## ğŸ“Š Monitoring & Cost Management

### Token Usage Tracking

```python
# app/llm/token_counter.py
import tiktoken

class TokenCounter:
    def __init__(self, model: str = "gpt-4"):
        self.encoding = tiktoken.encoding_for_model(model)

    def count_tokens(self, text: str) -> int:
        return len(self.encoding.encode(text))

    def estimate_cost(self, prompt_tokens: int, completion_tokens: int, model: str) -> float:
        # Pricing as of 2025 (example)
        pricing = {
            "gpt-4-turbo": {"prompt": 0.01 / 1000, "completion": 0.03 / 1000},
            "gpt-3.5-turbo": {"prompt": 0.0005 / 1000, "completion": 0.0015 / 1000},
            "claude-3-opus": {"prompt": 0.015 / 1000, "completion": 0.075 / 1000}
        }

        if model not in pricing:
            return 0.0

        cost = (
            prompt_tokens * pricing[model]["prompt"] +
            completion_tokens * pricing[model]["completion"]
        )

        return cost
```

---

This is a complete LLM_SERVICE.md specification with all major components, APIs, RAG implementation, tool calling, and deployment guides!
